{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1dK8-JmqQqf"
      },
      "source": [
        "## Milestone Project Food 101 Vision Big"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XlLdo3gZqZjF"
      },
      "source": [
        "## Check GPU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "605rSFfhqePe"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "!pip install -U -q tf-nightly\n",
        "\n",
        "# Check TensorFlow version (should be minimum 2.4.0+ but 2.13.0+ is better)\n",
        "import tensorflow as tf\n",
        "print(f\"TensorFlow version: {tf.__version__}\")\n",
        "\n",
        "# Add timestamp\n",
        "import datetime\n",
        "print(f\"Notebook last run (end-to-end): {datetime.datetime.now()}\")\n"
      ],
      "metadata": {
        "id": "RmJ-r_8NrfgG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Auiw6TdKtj1i"
      },
      "outputs": [],
      "source": [
        "# Get helper functions file\n",
        "import os\n",
        "\n",
        "if not os.path.exists(\"helper_functions.py\"):\n",
        "    !wget https://raw.githubusercontent.com/mrdbourke/tensorflow-deep-learning/main/extras/helper_functions.py\n",
        "else:\n",
        "    print(\"[INFO] 'helper_functions.py' already exists, skipping download.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dQl11oi_t0S8"
      },
      "outputs": [],
      "source": [
        "# Import series of helper functions for the notebook (we've created/used these in previous notebooks)\n",
        "from helper_functions import create_tensorboard_callback, plot_loss_curves, compare_historys\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2dZ9EXPIufG7"
      },
      "outputs": [],
      "source": [
        "# Get TensorFlow datasets\n",
        "import tensorflow_datasets as tfds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-hWp4e0vONO"
      },
      "outputs": [],
      "source": [
        "# List all available datasets\n",
        "datasets_list = tfds.list_builders()\n",
        "print(datasets_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ibmc-mUSvaVs"
      },
      "outputs": [],
      "source": [
        "(train_data, test_data), ds_info = tfds.load(name='food101',\n",
        "                                             split=['train','validation'],\n",
        "                                             shuffle_files=True,\n",
        "                                             as_supervised=True,\n",
        "                                             with_info=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y_zr8itywB8h"
      },
      "outputs": [],
      "source": [
        "ds_info.features"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the classnames\n",
        "class_names = ds_info.features['label'].names\n",
        "class_names[:10]"
      ],
      "metadata": {
        "id": "78L0V4pFr6HX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploring the food101 data from tfds\n",
        "\n",
        "To become one with our data we want to find\n",
        "* Class names\n",
        "* The shape of our input data (image tensors)\n",
        "* The datatype of our input data\n",
        "* What the labels look like (e.g. are they one-hot encoded or are they label encoded?)\n",
        "* Do the labels match up with the class names?\n"
      ],
      "metadata": {
        "id": "S4Y06YUxsO9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Take one sample of our training data\n",
        "for image, label in train_data.take(1):\n",
        "  print(f\" Image shape: {image.shape} \\nImage datatype: {image.dtype}, \\nTarget class from Food101 (tesnor form): {label}, \\nClass name (str form): {class_names[label.numpy()]}\")"
      ],
      "metadata": {
        "id": "TvzwGIEisapH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What does our image tensor look like?\n",
        "image"
      ],
      "metadata": {
        "id": "9sM98Ga1tEcs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "tf.reduce_min(image), tf.reduce_max(image)"
      ],
      "metadata": {
        "id": "maAz2CTkuc8q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Plot an image for tensorflow datasets"
      ],
      "metadata": {
        "id": "tThSe2dvulZO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Create a figure with 10 subplots (2 rows, 5 columns)\n",
        "plt.figure(figsize=(15, 6))  # Adjust figure size as needed\n",
        "\n",
        "# Take 10 images from train_data\n",
        "for i, (image, label) in enumerate(train_data.take(10)):\n",
        "    # Create subplot - i+1 because subplot indexing starts at 1\n",
        "    plt.subplot(2, 5, i+1)\n",
        "    plt.imshow(image)\n",
        "    plt.title(class_names[label.numpy()])\n",
        "    plt.axis('off')  # Optional: hide axes\n",
        "\n",
        "plt.tight_layout()  # Adjust spacing between plots\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0GsiDivZu2j-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create preproccesing functions for our data"
      ],
      "metadata": {
        "id": "cyp6wDuzvH2X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize_img(image, label):\n",
        "  \"\"\"Normalizes images: `uint8` -> `float32`.\"\"\"\n",
        "  # Resize the image before normalizing\n",
        "  image = tf.image.resize(image, [224, 224])\n",
        "  # Cast to float32 and normalize to [0, 1]\n",
        "  image = tf.cast(image, dtype=tf.float32)\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "5PmfiyL-xsaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed_img = normalize_img(image,label)[0]\n",
        "print(f\"Image before preprocessing:\\n {image[:2]}..., \\nShape:{image.shape}, \\nDatatype: {image.dtype}\\n\")\n",
        "print(f\"Image after preprocessing:\\n{preprocessed_img[:2]}...,\\nShape: {preprocessed_img.shape}, \\nDatatype: {preprocessed_img.dtype}\")"
      ],
      "metadata": {
        "id": "tW-JnFuTylq4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Batch and prepare datasets\n",
        "\n",
        "We're now going to make our data input pipeline run really fast."
      ],
      "metadata": {
        "id": "Rf1GsXG8zmDE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = train_data.map(map_func=normalize_img, num_parallel_calls=tf.data.AUTOTUNE)\n",
        "# Shuffle train_data and turn it into batches and prefetch it (load it faster)\n",
        "train_data = train_data.shuffle(buffer_size=1000).batch(batch_size=32).prefetch(buffer_size=(tf.data.AUTOTUNE))\n",
        "\n",
        "# Map preproccessing function to test data\n",
        "test_data = test_data.map(normalize_img, num_parallel_calls=tf.data.AUTOTUNE).batch(32).prefetch(tf.data.AUTOTUNE)\n"
      ],
      "metadata": {
        "id": "q_ITvhYE6u-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data, test_data"
      ],
      "metadata": {
        "id": "m2L9axt57OzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create modelling callbacks\n",
        "* ModelCheckpoint callback to save our model's progress after feature extraction."
      ],
      "metadata": {
        "id": "cJiu_HruAeI5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create tensorboard callback (import from helper functions.py)\n",
        "from helper_functions import create_tensorboard_callback\n",
        "\n",
        "checkpoint_path = 'model_checkpoint/cp.weights.h5'\n",
        "model_checkpoint = tf.keras.callbacks.ModelCheckpoint(checkpoint_path,\n",
        "                                                      monitor='val_acc',\n",
        "                                                      save_best_only=True,\n",
        "                                                      save_weights_only=True,\n",
        "                                                      verbose=0)"
      ],
      "metadata": {
        "id": "iVAhSC0AOxgM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup mixed precison training"
      ],
      "metadata": {
        "id": "Uu58yuYCPjcc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Turn on mixed precision training\n",
        "from tensorflow.keras import mixed_precision\n",
        "mixed_precision.set_global_policy(\"mixed_float16\")"
      ],
      "metadata": {
        "id": "JnyZl2hSTDag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mixed_precision.global_policy()"
      ],
      "metadata": {
        "id": "44E7tJVhTZ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## Build feature extraction model\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Create a base model\n",
        "input_shape = (224,224,3)\n",
        "base_model = tf.keras.applications.EfficientNetB0(include_top=False)\n",
        "base_model.trainable = False\n",
        "\n",
        "# Create functional model\n",
        "inputs = layers.Input(shape=input_shape, name='input_layer')\n",
        "x = base_model(inputs, training=False)\n",
        "x = layers.GlobalAveragePooling2D()(x)\n",
        "x = layers.Dense(len(class_names))(x)\n",
        "outputs = layers.Activation(\"softmax\", dtype=tf.float32, name='softmax_float32')(x)\n",
        "model = tf.keras.Model(inputs, outputs)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(),\n",
        "              metrics=['accuracy'])\n"
      ],
      "metadata": {
        "id": "34eM2IywUyYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "lUuMckFQWUJD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, layer in enumerate(model.layers):\n",
        "  print(layer, layer.dtype, layer.trainable, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "7z0LmPqyYhwY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the dtype_policy attributes for layers in base_model\n",
        "for layer in model.layers[1].layers:\n",
        "    # print(layer, layer.dtype, layer.trainable, layer.dtype_policy)\n",
        "    if layer.dtype_policy.name == \"float32\":\n",
        "      print(layer, layer.dtype, layer.trainable, layer.dtype_policy)"
      ],
      "metadata": {
        "id": "RO1fqC-DYwaH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history_101_food_classses_feature_extract = model.fit(train_data,\n",
        "          epochs=3,\n",
        "          steps_per_epoch=len(test_data),\n",
        "          validation_data=test_data,\n",
        "          validation_steps=int(0.15 * len(test_data)),\n",
        "          callbacks=[model_checkpoint])"
      ],
      "metadata": {
        "id": "jwFtiv0ICYHO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results_feature_extracts_model = model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "w84bj2-7FPlx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=3,\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")"
      ],
      "metadata": {
        "id": "kPt6wEQZMdQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "base_model.trainable = True"
      ],
      "metadata": {
        "id": "GK4GI1btHNz5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for layer in base_model.layers[:-10]:\n",
        "  layer.trainable = False"
      ],
      "metadata": {
        "id": "3C7ikfWPHzTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "initial_epoch = 3\n",
        "last_epoch = history_101_food_classses_feature_extract.epoch[-1]\n",
        "last_epoch"
      ],
      "metadata": {
        "id": "mTcrs8VmH_xn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer=tf.keras.optimizers.Adam(learning_rate=0.0001),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "ms9z2FiTNREp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_epochs = initial_epoch + 3\n",
        "fine_tune_history_1 = model.fit(train_data,\n",
        "                                epochs=100,\n",
        "                                validation_data = test_data,\n",
        "                                validation_steps= int(0.15 * len(test_data)),\n",
        "                                initial_epoch=last_epoch,\n",
        "                                callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "iOVERAqtIdT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fine_tune_results = model.evaluate(test_data)"
      ],
      "metadata": {
        "id": "BT8SxnuDJOjO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}